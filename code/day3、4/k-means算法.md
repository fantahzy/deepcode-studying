# **K-Means算法**

## 算法思想

选取一个点，数据距离这个点的欧氏距离越小，数据的相似度越高，将一个点的思想扩展到进行多个点（中心点），比较更多数据与这些选取的中心点的欧氏距离，对这些数据进行划分，根据相似度得出最优数据。



## 算法公式

对于给定的一个包含n个d维数据点的数据集X以及要分得的类别K,选取欧式距离作为相似度指标，聚类目标是使得各类的聚类平方和最小，即最小化：

 　　　　　　                       　![img](https://images0.cnblogs.com/blog2015/771535/201508/071351008301642.jpg)

结合最小二乘法和拉格朗日原理，聚类中心为对应类别中各数据点的平均值，同时为了使得算法收敛，在迭代过程中，应使最终的聚类中心尽可能的不变。

## 基本的步骤为：

step1：选定要聚类的类别数目k（如上例的k=3类），选择k个中心点。

step2：针对每个样本点，找到距离其最近的中心点（寻找组织），距离同一中心点最近的点为一个类，这样完成了一次聚类。

step3：判断聚类前后的样本点的类别情况是否相同，如果相同，则算法终止，否则进入step4。

step4：针对每个类别中的样本点，计算这些样本点的中心点，当做该类的新的中心点，继续step2。

## **循环迭代式的算法**

1. 初始化

   随机选择K个点，初始化中心点

2. 交替更新

   计算每个点到中心的距离，最近的距离就分配给那个中心点

   计算更新每个类之中的中心点

3. 直到中心点不再发生改变



##  优缺点

K-Means的主要优点有：

　　　1）原理比较简单，实现也是很容易，收敛速度快。

　　　2）聚类效果较优。

　　　3）算法的可解释度比较强。

　　　4）主要需要调参的参数仅仅是簇数k。

K-Means的主要缺点有：

　　　1）K值的选取不好把握

　　　2）对于不是凸的数据集比较难收敛

　　　3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。

　　　4） 采用迭代方法，得到的结果只是局部最优。

　　　5） 对噪音和异常点比较的敏感。